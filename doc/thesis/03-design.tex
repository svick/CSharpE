\chapter{Design}

Before starting actual design of the system, its name should be decided. This name would be used in names of namespaces, assemblies, NuGet packages and so on, so it should fit well with their requirements and conventions. The name should also be reasonably unique, easy to remember and not too long. The name chosen based on these principles is "CSharpE", meaning "C\#, extensible".

As explained in the previous chapter, this system has two main tasks: representing code and transforming code. This means it is natural to split the project into two main parts: \cs{CSharpE.Syntax} and \cs{CSharpE.Transform}, respectively.

\section{CSharpE.Syntax}
\label{syntax-design}

The \cs{CSharpE.Syntax} namespace contains all the types necessary for representing and modifying C\# code starting from the project level and going down all the way to the expression level. There is no representation for solutions, because extensions work at the project level, which means solutions are not necessary.

\subsection{Example}

Before explaining details about the design of this namespace, it might be useful to see an example of its usage. Listing~\ref{csharpesyntax-example} shows how the running example from Section~\ref{running-example} could be implemented using this \ac{API}. Notice how the code is very short, when compared with the previously shown examples from Chapter~\ref{background}, without resorting to a custom language that mixes two programs, like \ac{T4} does. This is thanks to following the guidelines from Section~\ref{representing-code}.

\begin{listing}
\inputminted[firstline=15,lastline=31]{csharp}{samples/CSharpE.Syntax/Program.cs}
\caption{CSharpE.Syntax example}
\label{csharpesyntax-example}
\end{listing}

Specifically, what the code does is to create a project (\cs{Project}) containing one source file (\cs{SourceFile}) that contains the initial code. Then it finds classes (\cs{ClassDefinition}) in the project and for each of them, adds \cs{IEquatable<T>} to the list of base types and also changes all its fields (\cs{FieldDefinition}) to auto-implemented properties (\cs{PropertyDefinition}).

The rest of this chapter describes the \ac{API} in detail.

\subsection{General principles}

There are some rules that apply though all levels of this \ac{API}:

\begin{itemize}
	
\item Types in this \ac{API} are regular mutable classes.

\item Types that represent nodes in the C\# syntax tree inherit from the common base class \cs{SyntaxNode}. This includes most types from the source file level down.

\label{no-parent-syntax}
A syntax node can have only one parent node, to make sure the syntax tree is actually a tree and so that mutating a node does not affect another, seemingly unrelated part of the syntax tree. But this parent node is not exposed in the \ac{API}, for reasons explained in Section~\ref{no-parent-transform}.

Syntax nodes can be deep cloned by calling the \cs{Clone} method, which is a generic extension method. It is that way to avoid having to cast its result to the correct type, which would be necessary if \cs{Clone} was a simple instance method on \cs{SyntaxNode}. Syntax nodes are also cloned instead of assigning a new parent node, to maintain the tree shape.

\item Collections of nodes are usually exposed as properties of type \cs{IList<T>}. This interface is the most flexible out of the commonly used collection interfaces in .NET. Using an interface means that the user is shielded from the implementation detail of which specific collection type is used.

Using this interface, which is implemented by commonly used types like arrays or \cs{List<T>}, also means that collection properties can have easily usable setters. For example, to set the body of a method to a specific sequence of statements, one could use code similar to the following: \cs{method.Body.Statements = new Statement[] { ... };}.

For convenience, some collections are also exposed on higher levels as methods. For example, \cs{TypeDefintion} contains the \cs{Methods} property, while \cs{SourceFile} and \cs{Project} contain the \cs{GetMethods} method, which returns methods from the whole file or project, respectively. Such methods return \cs{IEnumerable<T>}, because it does not make sense to for example add a method directly to a file or a project, it has to be added to a specific type.

\item The \ac{API} includes implicit conversion operators when appropriate. Specifically, they can be used to convert from a definition to a reference (e.g. from \cs{TypeDefinition} to \cs{NamedTypeReference}) or from a node to a simple wrapper for that node (e.g. from \cs{Expression} to \cs{ExpressionStatement}).

Using implicit conversions makes code shorter, but also harder to understand, because it is an operation that is not visible in the code. For this reason, all implicit conversion operators have an alternative form, usually a constructor of the target type.

\item There is a \cs{SyntaxFactory} type, which exists to make creating syntax nodes more succinct, when combined with \cs{using static}. For example, it means that to use the \cs{this} keyword, it is possible to write just \cs{This()}, instead of  \cs{new ThisExpression()}.

\end{itemize}

Listing~\ref{syntax-types} shows inheritance hierarchy of the \cs{CSharpE.Syntax} namespace.

\begin{listing}
	
\dirtree{%
.1 .
.2 Project.
.2 NamespaceOrTypeDefinition.
.2 SyntaxNode.
.3 SourceFile.
.3 NamespaceDefition.
.3 MemberDefinition.
.4 BaseTypeDefinition.
.5 TypeDefinition.
.6 ClassDefinition.
.6 StructDefinition.
.6 InterfaceDefinition.
.5 EnumDefinition.
.5 DelegateDefinition.
.4 BaseMethodDefinition.
.5 MethodDefinition.
.5 ConstructorDefinition.
.5 FinalizerDefinition.
.5 OperatorDefinition.
.4 BasePropertyDefinition.
.5 PropertyDefinition.
.5 IndexerDefinition.
.4 FieldDefinition.
.4 EventDefinition.
.3 Statement.
.3 Expression.
}

\caption{Inheritance hierarchy of commonly used types in the \cs{CSharpE.Syntax} namespace}
\label{syntax-types}
	
\end{listing}


\subsection{Projects}

At the top of this \ac{API} is the \cs{Project} class, which represents a collection of C\# source files and library references. It also contains helper methods for accessing types from all files within a project, for extensions that do not care about which file contains which type.

The \cs{Project} class is also point of interoperation between this \ac{API} and Roslyn: \cs{Project} can be constructed from a \cs{CSharpCompilation} and it also exposes a \cs{CSharpCompilation} as a property.

\subsection{Source files}

Each C\# source file in the project is represented as an instance of the \cs{SourceFile} class. A source file has name, text and a list of members, which are namespace and type definitions.

\medskip

The obvious object-oriented way to model this list would be to have a common base class shared by the namespace definition class and the type definition class. The problem with this is that types can be members of other types, while namespaces cannot. So, we would want type definition to inherit from the member definition class (along with other kinds of type members), and namespace definition to not inherit from that class. But this is not possible, because .NET does not allow multiple inheritance of classes.

There are several options of how to resolve this:

\begin{itemize}
\item Make the common base class of namespace definition and type definition into an interface, since multiple inheritance of interfaces is allowed. This interface would not be very useful on its own, since namespace and type definitions are not very similar and generally require different processing.
\item Make namespace definition inherit from the member definition class. This would allow adding namespace definitions as members of types, which is not valid C\#, so it is undesirable.
\item Don't have namespace definitions in the syntax tree at all, instead the namespace of a type definition is surfaced as a property. This is how namespaces are represented in \ac{IL} and Reflection. The problem with this is that deviates from the syntax of C\# too much, which could be confusing to users.
\item Use a different class for representing nested type definitions. This way, there is no issue with multiple inheritance, because each base class would be inherited by a different class. The problem here is that users will probably expect that nested types behave the same as regular types, since both have the same syntax in C\#.
\item Instead of a common base type, use a discriminated union. The biggest issue with this approach is that it is not commonly used in object-oriented design and so it might be unfamiliar to C\# programmers and it also would not fit well with the rest of this object-oriented \ac{API}.
\end{itemize}

The option chosen from the above was to use discriminated union \cs{struct} called \cs{NamespaceOrTypeDefinition}, because it fits the best with the expected usage pattern of these types, where nested types and regular types are handled the same, while namespaces and types are handled in separate code paths.

\medskip

Notice that \cs{using} directives are not exposed in the \ac{API} at all, because they are managed automatically. This makes the \ac{API} easier to use, at the cost of preventing users from choosing which syntax should be used, which is consistent with the principles outlined in the previous chapter.

\subsection{Types}

The types that can be defined in C\# are classes, structs, interfaces, enums and delegates. Classes, structs and interfaces are very similar in that all three can be generic, have a list of base types and have various members. Also, especially when it comes to classes and structs, it could make sense fairly often to manipulate them in the same way. 

This means that a reasonable design would be to have the types for classes, structs and interfaces inherit from a common base class and this base class, along with the types for enums and delegates, should inherit from another base class. The problem with this design is naming: there is no established name that would apply to classes, structs and interfaces, but not to enums and delegates. For this reason, the names chosen for the two base classes are \cs{TypeDefinition} and \cs{BaseTypeDefinition}.

\medskip

For a \cs{TypeDefinition}, all its members are contained in the \cs{Members} list, while various kinds of members like fields or methods are also contained in their own lists (for example, \cs{Fields} or \cs{Methods}). These smaller lists are kept synchronized with the main list of members. This means these lists effectively act as filtered views on the main list.

To make adding new members easier, \cs{TypeDefinition} also contains methods like \cs{AddFields}. For example, consider the following code:

\begin{minted}{csharp}
var field = new FieldDefinition(...);
type.Fields.Add(field);
\end{minted}

By using the \cs{AddFields} method, that code can be simplified to:

\begin{minted}{csharp}
var field = type.AddField(...);
\end{minted}

\subsection{Members}

Class, struct and interface definitions can contain various kinds of members, namely fields, methods, properties, events, indexers, operators, constructors, finalizers (also known as destructors) and nested type definitions. Not all kinds of members are valid for all three kinds of types, but since invalid members might be useful to some extensions, the \ac{API} still allows them.

All kinds of members can have modifiers. While in source, member modifiers can be defined in different order (for example, both \cs{public static} and \cs{static public} are valid modifiers for a method), this order does not make a difference. For this reason, this \ac{API} represents all modifiers using a single flags enum, \cs{MemberModifiers}. 

Because manipulating flags enums using bitwise operators can be cumbersome, the \ac{API} also includes helper read-write properties for most valid modifiers for each kind of member. The exception to this are access modifiers. Because a member can have only one kind of declared accessibility (which includes all the access modifiers on their own and also the combinations \cs{protected internal} and \cs{private protected}), access modifiers are surfaced as a read-write property named \cs{Accessibility}, along with a read-only property for each kind of declared accessibility.

\medskip

Even though the list of kinds of members above might make it seem like the mapping between syntactic forms of members from the C\# specification and types in this \ac{API} should be obvious, it is not. The cases that are not obvious are:

\begin{itemize}
\item Constants, fields and field-like events can define more than one member with each declaration, for example, \cs{private int x, y;}. With considered designs, this ability makes processing all declarations of these kinds significantly harder, even those that declare just one member. For this reason, each member defined in this way is actually represented as a distinct instance of the appropriate type. For example, the previously shown declaration would be represented as two instances of the \cs{FieldDefinition} type.

\item The specification lists constants separately from fields. But since constants are very similar to fields with the \cs{const} modifier, that is how they are represented: both are \cs{FieldDefinition}, the difference is only in modifiers. This also makes changing a field to a constant or vice versa easier.

\item The specification lists two syntactic forms of events: field-like events and events with accessors. Since both define the same kind of member, they are both represented as \cs{EventDefinition}.

This is made easier by splitting a single declaration with multiple definitions into separate instances, as mentioned before, because otherwise the same type would have to represent quite distinct values: a field-like event declaration that has no accessors, but could define more than one event; and a declaration of an event with accessors, which always defines exactly one event.

\item The specification lists three syntactic forms of operators: unary operators, binary operators and conversion operators. It would be possible to have a separate type for each form, but a simpler solution is to have just one type, \cs{OperatorDefinition}, which represents all three forms. An \cs{OperatorDefinition} then has a kind, which is \cs{Implicit} or \cs{Explicit} for conversion operators, or the name of one of the overloadable unary or binary operators (for example, \cs{Addition} or \cs{Xor}).

\item The specification lists instance constructors and static constructors separately. But since \cs{Static} already exists as a member modifier for other kinds of members, it could be confusing if it did not work for constructors. For this reason, both instance and static constructors are represented as \cs{ConstructorDefinition}.

\item The Microsoft C\# specification calls the member that is invoked before an instance is garbage collected "destructor". But the Ecma C\# specification\footnotemark{} and the official documentation~\cite{finalizer} both call this member "finalizer", so that is the name used here.

\footnotetext{Quoting from \cite{ecma-334}:
\begin{quote}
In an earlier version of this standard, what is now referred to as a "finalizer" was called a "destructor". Experience has shown that the term "destructor" caused confusion and often resulted to incorrect expectations, especially to programmers knowing C++. In C++, a destructor is called in a determinate manner, whereas, in C\#, a finalizer is not. To get determinate behavior from C\#, one should use \cs{Dispose}. 	
\end{quote}}

\end{itemize}

\medskip

Another set of decisions to make are about the inheritance hierarchy of member definitions:

\begin{itemize}
\item Classes for all kinds of member definitions derive from a common base class, \cs{MemberDefinition}. This includes type definitions, to support nested types, as has been described in previous sections.
\item Methods, constructors, destructors and operators all have parameters and a body, so they inherit from the \cs{BaseMethodDefinition} class. This could make processing method-like definitions easier.
\item Properties and indexers both have a type and also \cs{get} and \cs{set} accessors, so they inherit from \cs{BasePropertyDefinition}. Events also have a type and accessors, but they are a different set of accessors (\cs{add} and \cs{remove}), so they are not included.
\end{itemize}

\todo{Bodies}

\subsection{Statements}

\subsection{Expressions}

\subsection{References}

\section{CSharpE.Transform}
\label{transform-design}

Now that the \ac{API} for representing and modifying code has been designed, it is time to decide how is the system going to transform a project for some extension. Per Section~\ref{extensions-are-dotnet-libraries}, extensions are distributed as .NET libraries. This means the system will need some way to recognize what transformations an extension wants to perform and then execute them.

A reasonable way to do this is to have an interface that is implemented by each transformation. The system would then create an instance of the type (or types) that implements this interface and call a method on it whenever the transformation needs to be performed.

As has been explained before, the transformations that need to be performed can be different at design time and at build time. Also, different extensions will have different relationships between the two transformations. To fulfill these two requirements, the interface will include a parameter that specifies which transformation should be performed and there will also be abstract classes that hide this parameter for transformations that do not need it. This way, full flexibility of using an interface is maintained, while recommended patterns are communicated using the abstract classes.

\medskip

Listing~\ref{itransformation-api} shows the \ac{API} of this interface (\cs{ITransformation}) and the related abstract classes: 

\begin{itemize}
\item The base class \cs{Transformation}, which is a convenient location for helper members that are going to be useful in many transformations. Specifically, the \cs{NotImplementedStatement} property should be useful in most transformations that have a design-time component, as the body of generated members.
\item The \cs{SimpleTransformation} class, which can be used for transformations that behave the same at design-time and at build-time.
\item The \cs{BuildTimeTransformation} class, which can be used for transformations that do not do anything at design-time, for example, aspects.
\end{itemize}

\begin{listing}
\begin{minted}{csharp}
public interface ITransformation
{
    void Process(Project project, bool designTime);
}

public abstract class Transformation : ITransformation
{
    public abstract void Process(Project project, bool designTime);

    protected static Statement NotImplementedStatement { get; }
}

public abstract class SimpleTransformation : Transformation
{
    protected abstract void Process(Project project);
}

public abstract class BuildTimeTransformation : Transformation
{
    protected abstract void Process(Project project);
}
\end{minted}
\caption{Simplified \ac{API} of types used to implement transformations}
\label{itransformation-api}
\end{listing}

Listing~\ref{csharpetransform-simple-example} shows how the running example from Section~\ref{running-example} could be implemented as a transformation. Notice that it uses one of the aforementioned abstract classes and that the body of the method is the same as the main part of Listing~\ref{csharpesyntax-example}.

\begin{listing}
\inputminted[firstline=9,lastline=27]{csharp}{samples/CSharpETransform.Simple/EntityTransformation.cs}
\caption{CSharpE.Transform simple example}
\label{csharpetransform-simple-example}
\end{listing}

\subsection{Making transformations efficient}

A much more complicated question is: how to make design-time transformations efficient, by regenerating only code that depends on code modified by the user? As can be seen from the previous example, transformations are often structured around \cs{foreach} loops over syntax nodes: the transformation applies for each class, or for each type with an attribute, or for each method. And this includes \cs{foreach} loops that are hidden by convenience methods from \cs{CSharpE.Syntax}. For example, consider the following code iterating over all methods in a project:

\begin{minted}{csharp}
foreach (var method in project.Methods())
{
   ...
}
\end{minted}

The above code is effectively the same as the following code, which does not use convenience methods:

\begin{minted}{csharp}
foreach (var file in project.SourceFiles)
{
    foreach (var type in file.GetTypes())
    {
        foreach (var method in type.Methods)
        {
            ...
        }
    }
}
\end{minted}

This, combined with the fact that a programmer editing their code usually focuses at a single higher-level syntax node (like a method or a class) at a time, makes \cs{foreach} loops ideal to decide whether transformation code should be executed again: the system could make this decision for each iteration of a \cs{foreach} loop, based primarily on whether the syntax node that is being processed by that iteration changed.

Bu the regular \cs{foreach} loop does not offer sufficient flexibility for the kind of operations that will be required to implement this "smart" \cs{foreach} loop, so an alternative has to be considered.

One option would be to create a transformation for writing transformations: the author of a transformation writes a regular \cs{foreach} loop, which is then translated into a smart loop. The problem with this approach is that it hides what should be explicit: as explained in detail below, there are significant differences between the behavior of a regular \cs{foreach} loop and a smart loop, which should not be hidden from the transformation author.

Instead, a method with a lambda parameter will be used, similar to the \cs{Parallel.ForEach} method in .NET. \cite{parallel-foreach}

\medskip

To make this more concrete, Listing~\ref{csharpetransform-smart-example} shows how the smart \cs{foreach} loop could be used in the running example. Notice that the outer \cs{foreach} loop from Listing~\ref{csharpetransform-simple-example} was replaced with a call to the \cs{Smart.ForEach} method, with a lambda serving as the loop body. Why the inner loop was not similarly changed will be explained at a later point.

\begin{listing}
\inputminted[firstline=9,lastline=27]{csharp}{samples/CSharpETransform.Smart/EntityTransformation.cs}
\caption{Example of CSharpE.Transform smart \cs{foreach} loop}
\label{csharpetransform-smart-example}
\end{listing}

\medskip

In general, how this works is that each loop iteration has its own inputs and outputs. Inputs are the syntax nodes and other data that the iteration has access to. Outputs are any changes made to those syntax nodes and also any data it returns (while a regular \cs{foreach} loop cannot return anything, a \cs{foreach}-like method can).

When an iteration of the loop is executed for the first time, its body is invoked as if it was a regular \cs{foreach} loop, but at the same time, its inputs and outputs are recorded. When it is executed again, the new inputs are compared with the recorded ones. If they match, instead of invoking the loop body, the recorded outputs are used. This way, if only a small part of the input changes, there is a good chance that only small part of the transformation will have to be executed again.

\medskip

As an example, consider a project with two files, each of which contains two classes and a transformation that uses a smart \cs{foreach} loop over the project's classes. Since the loop over classes in a project includes a hidden loop over files in the project, the first time this transformation is ran, an iteration of the first loop will be executed for each of the two files and an iteration of the second loop will be executed for each of the four classes.

If the second class in the second file is modified and the transformation is ran again, the first iteration of the first loop will not be executed, because nothing in the first file changed. The previously recorded outputs will be used for this iteration. The second iteration will be executed, because the second file changed. The iteration for the first class in the second file will not be executed, because the code of this class did not change. The only class for which any code from the transformation will be executed is the one that changed: the second class of the second file.

Listing~\ref{csharpetransform-log} shows the output of internal logging for this scenario. Each line represents a section of code to be executed. The first entry on each line is the name of the type of the object that is being processed. The second entry is the name of the object (empty in the case of a project). The last entry indicates whether the section of code was actually executed: \cs{transform} means that it was executed, \cs{cached} means that it was not executed and that its recorded outputs were used.

\begin{listing}
\begin{minted}{text}
(TransformProject, , transform)
(SourceFile, File1.cs, transform)
(ClassDefinition, Class1A, transform)
(ClassDefinition, Class1B, transform)
(SourceFile, File2.cs, transform)
(ClassDefinition, Class2A, transform)
(ClassDefinition, Class2B, transform)

(TransformProject, , transform)
(SourceFile, File1.cs, cached)
(SourceFile, File2.cs, transform)
(ClassDefinition, Class2A, cached)
(ClassDefinition, Class2B, transform)
\end{minted}
\caption{Log output for running a transformation twice, with a change between the runs}
\label{csharpetransform-log}
\end{listing}

\subsection{Smart loop details}

When it comes to designing how exactly the smart \cs{foreach} loop should look like, there are several considerations:

\begin{itemize}
\item The system assumes that code in a loop iteration body is deterministic and free of unrecognized side-effects.

For smart loops to work correctly, the output of each loop iteration has to depend only on its inputs and all its outputs have to be recognized by the system. Otherwise, when the recognized inputs stay the same, the iteration is not re-executed, and the end result is different than what it would be without a smart loop.

This does not mean non-determinism or side-effects are completely forbidden in transformations, but it does mean they are severely limited. Some examples:

\begin{itemize}
\item If an iteration needs access to the current date, it should not read the \cs{DateTime.Today} property itself. Instead, its value can be passed as an input.
\item If an iteration logs what it does to a file, it might be acceptable that the log file is only updated when the inputs change.
\item If an iteration updates some global cache, then that is technically a side-effect. But if that cache is only used to make the code more efficient and no code relies on it for correctness, then this side-effect does not cause any issues.
\item If an iteration uses a randomized algorithm, the fact that the output of the algorithm could change with each execution is generally not a positive feature. In this case, skipping re-execution should be acceptable.
\item Mutating the syntax node that is being processed by the iteration, including its child nodes, is explicitly allowed. This is because any changes to that node are recognized by the system and are re-applied even if execution of the iteration is skipped.
\end{itemize}

Because nondeterminism and unrecognized side-effects can sometimes be desirable, and because .NET does not have a good way of recognizing them, the system does not have any mechanism of preventing them.

Also note that nondeterminism might be undesirable in a transformation, even when ignoring its effects on smart loops. This is because a transformation can decide what code is valid and how the code behaves when executed. So, code that compiles and works correctly now could stop compiling or it could change its behavior in the future, which is generally undesirable.

\item Access to syntax nodes has to be controlled.

\nopagebreak

When some syntax node is accessed by a loop iteration, and this syntax node is changed between two executions of that iteration, the iteration has to be re-executed, because that change could alter its output. While it would be possible to track which syntax nodes are accessed by an iteration, the system described in this work does not attempt to do so, on the assumption that it is not necessary to achieve sufficient performance for design-time transformations.

\label{no-parent-transform}
Instead, whenever any syntax node accessible by a loop iteration changes, that iteration is re-executed. As a consequence of this, syntax node objects do not have parent references, as has been pointed out in Section~\ref{no-parent-syntax}. If such references did exist, the whole file or even the whole project would be accessible from any syntax node, which means even a small change would cause re-execution of large amounts of code.

Instead of providing no parent references whatsoever, another option would be to have parent references on all node types, but prohibit accessing "dangerous" parents at runtime (attempting to do so could result in throwing an exception or returning \cs{null}). But because this could be confusing to users and because parent references are not necessary, this option was not chosen.

Another consequence of these node accessibility rules is that care has to be taken about which other nodes, apart from the one being processed by the loop iteration, are accessible. This is considered along with other kinds of input in the next point.

\item All data that is passed in has to be tracked.

\nopagebreak

The system has to understand all inputs of a loop iteration. This includes the syntax node that is being processed by the iteration, but also any other data. For that data, the system has to:

\begin{itemize}
\item understand when they change, so that it can correctly decide when to re-execute the loop,
\item ensure loop iterations do not mutate them, which would be considered an unrecognized output,
\item deep clone them, so that further mutations after the smart loop do not affect following executions.
\end{itemize}

For these reasons, the system strictly controls which data types are allowed as additional inputs. The allowed types are: primitive types (such as \cs{int} and \cs{bool}), \cs{string}, delegates without closures, syntax nodes, and also tuples, arrays and lists of allowed types. This set of types should be sufficient for most use cases, but if it turns out it is not, it can be easily extended in the future.

\medskip

The natural way of passing additional data into a lambda is by directly accessing variables from the outer scope. When such lambda is compiled, any variables from the outer scope that it accesses are stored in a closure object, that then becomes part of the delegate that represents the lambda.

The system could attempt to recognize this closure object as part of the inputs to a smart loop, but this approach has several issues:

\begin{itemize}
\item Depending on how exactly the compiler decided to form the closure object, it could include more variables than necessary.

For example, consider the following code:

\begin{minted}{csharp}
int a = 1, b = 2;

SomeMethod(() => a);
SomeMethod(() => b);
\end{minted}

The current version of the compiler creates a single shared closure object for both lambdas, which means inspecting the closure object for either lambda will reveal fields for both local variables.

For smart loops, this would mean that the system would have to assume that all fields in the closure object are inputs, which would lead to unnecessary re-executions when a variable is included in the closure object, but is not actually used by the lambda.

\item The exact shape of the closure object is an implementation detail of the compiler, which the system would have to understand.

There is no specification that governs what shape closure objects created by the compiler should have. The compiler can behave differently in different situations and its behavior can also change between versions.

The system would need to have a fairly deep understanding of this behavior, so that it could understand when the data changes, ensure it is not mutated by loop iterations and deep clone it, as explained previously.

\item It is not explicit.

In most cases, it is not important to understand which variables from the outer scope are accessed by a lambda in regular C\# code, which is why C\# does not have any built-in way of determining or controlling this (unlike for example C++, where the capture list has to be explicitly specified for each lambda).

But this is different for the lambda that forms the body of a smart loop, because each additional accessed variable could be the cause of re-execution, which would make understanding which variables are accessed by a lambda important.
\end{itemize}

For these reasons, passing additional data to a smart loop through closure objects is not allowed. In fact, the system checks that the lambda passed as the body of a smart loop does not access any variables from the outer scope and throws an exception if it does. This still relies on implementation details of the compiler,\footnote{The representation of lambdas that do not access any variables from the outer scope did indeed change in the past. With the compiler for C\# 5.0 or older, the closure object for such lambdas was \cs{null}. With the compiler for C\# 6.0 or newer, the closure object is instead an instance of a compiler-generated class with no instance fields. \cite{codegen-differences} The system accepts both approaches.} but to a lesser degree, so it should be less brittle than allowing passing data through closure objects.

Instead, additional data is passed as arguments to the \cs{Smart.ForEach} method, and accessed by the loop body as additional parameters to the lambda. One disadvantage of this approach is that it effectively requires having two different names for the same variable (one outside the loop, and one inside), because C\# does not allow variable shadowing within methods.

\item All data that is passed out has to be tracked.

Regular loops in C\# do not return values, instead, they often mutate some object from the outer scope (for example, an iteration could call \cs{Add} on a \cs{List<T>} that is declared outside the loop). Since this is not allowed according to the rules described in the previous point, an alternative approach that can be more easily controlled by the system would be very useful.

The approach used by the system is to allow the lambda that represents the body of the loop to return a value. The whole loop then returns a list of these values (specifically, \cs{IReadOnlyList<T>}), one for each iteration. When an iteration is not re-executed, the previously returned value is used again. To make sure further changes to returned values do not affect following executions, the values returned from loop iterations are deep cloned. For this reason, they also have the same limitation on allowed types as input values.

\item \label{smart-loop-collection} The system has to understand the collection that is being iterated.

To efficiently re-execute a smart loop, the system has to have a good understanding of what changed in the collection since the last execution. And because smart loops are often nested, either explicitly, or implicitly by using convenience methods that bypass one or more levels of syntax nodes (such as the previously mentioned \cs{project.Methods()}), it is not enough to determine when an item in a current version of the collection is exactly the same as an item in the previous version: it is also important to recognize when a current item is similar to an item from the previous version, so that results can be reused for the parts of the item that did not change.

Because computing these kind of differences for arbitrary collections is not trivial and generally should not be necessary in transformations, the system only allows performing smart loops on collections provided by the system. Since these collections satisfy some specific conditions (for example, a collection that only contains syntax nodes from the same file in source code order), computing their differences is fairly easy and the results should be quite accurate.

Also, collections that bypass levels of syntax nodes, which are returned by convenience methods, have to be understood by the system, so that a smart loop over one of them behaves the same as multiple nested smart loops, one for each level of syntax nodes. This way, determining which code does not have to be re-executed can happen at larger granularities.

An alternative design, which would avoid the need for recognizing these special collections, would be to have a smart loop method for each collection on each type of syntax node. For example, consider the following code:

\begin{minted}{csharp}
Smart.ForEach(project.GetClasses(), lambda);
\end{minted}

With this alternative design, it would be instead:

\begin{minted}{csharp}
project.ForEachClass(lambda);
\end{minted}

While this syntax might look appealing at first, the problem with it is that it would significantly increase the \ac{API} surface: the \cs{Smart.ForEach} method has several overloads (for varying number of arguments passed to the lambda and for returning a result collection) and this design would require adding those overloads for every collection on every type of syntax node. For this reason, the alternative design was rejected.

\item The smart loop methods should not be extension methods.

\nopagebreak

According to Framework Design Guidelines, extension methods should only be used when they work for every implementation of an interface. \cite{fdg-extension-methods} Since smart loops only work for collections provided by the system, making them extension methods would be a violation of the guidelines.

\end{itemize}

\subsection{Smart segment}

Now that we understand the limitations of smart loops, it should be clear why the second \cs{foreach} loop in Listing~\ref{csharpetransform-smart-example} cannot be a smart loop: it iterates over fields of a class, which means an iteration of a smart loop would only be allowed to mutate the field it is processing, when it needs to mutate the class itself by adding a property to it.

Because code like this, where a member is generated based on another existing member, is expected to be common, CSharpE.Transform contains a special method just for this purpose: It is called \cs{Smart.Segment} and it creates a segment of code that can add members to a type, but is strictly limited when it comes to interacting with the type in any other way. Specifically, this means that the code in such a segment does not depend on members of the type, which means it does not have to be re-executed when an unrelated member changes.

Listing~\ref{csharpetransform-smart-example2} shows how \cs{Smart.Segment} can be used in the running example. Notice that the original \cs{foreach} loop has been changed into a smart loop that reads relevant data from fields and a regular \cs{foreach} loop that adds properties through \cs{Smart.Segment}.

\begin{listing}
\inputminted[firstline=9,lastline=30]{csharp}{samples/CSharpETransform.Smart2/EntityTransformation.cs}
\caption{Example of CSharpE.Transform smart segment}
\label{csharpetransform-smart-example2}
\end{listing}

\section{User experience}

Another aspect of the system that should be considered is the experience of both creating an extension and using one or more extensions when writing code.

\medskip

Because a CSharpE extension is a regular .NET library that references the CSharpE.Transform library and implements the transformation interface, it is created using the same tools as any other .NET library. And just like most .NET libraries, extensions are distributed as NuGet packages and can use any supported distribution channel, including NuGet.org.

\medskip

The situation is more complicated for code that wants to use extensions. First, consider file extensions. CSharpE could require a custom file extension (like \cs{.cse}) for source files that have transformations applied to them. But that means fully adopting CSharpE in a project would require renaming all source files. Also, it could lead to a situation where some files in a project are using CSharpE and some are not, which could be confusing. For these reasons, CSharpE should use the same file extension as C\#: \cs{.cs}.

Second, build-time versions of extension transformations have to be executed whenever user code that uses extensions is built. This includes the user choosing to build their code from an \ac{IDE}, but also building on a build server, possibly as part of a \ac{CI} process. In both cases, the code will be built using MSBuild, which means a reasonable choice is to take advantage of its extensibility options by creating a custom "task" that will be executed before the build process invokes the C\# compiler. This custom task can be distributed through NuGet, making it easy to install alongside extensions.

Third, design-time versions of transformations have to be executed while the user is editing their code in an \ac{IDE} and the experience should be as close as possible to editing regular C\# code. Because every \ac{IDE} and code editor has their own mechanism for extending it, only Visual Studio will be supported.

\label{debugging}
Fourth, it should be possible to use debugging while developing extensions and code that is using extensions. Extension authors will want to debug extensions themselves and also user code after the build-time version of a transformation has been run. Extension users would likely prefer to see only the code they wrote, while code after build-time transformations is being debugged.

Debugging extensions is complicated by the fact that they run inside MSBuild and Visual Studio. To debug them, advanced debugging techniques have to be used, such as manually specifying debug executable, attaching the debugger to a running process or using \cs{Debugger.Break();} \cite{debugger-break} to launch the debugger when the transformation is executed. This is not ideal, and even though one of the goals of this system is to make developing extensions as easy as possible, it is likely extensions will be mostly developed by more experienced developers, so these hurdles should be acceptable.

Debugging user code after build-time transformations works thanks to the MSBuild custom task. This is because the task runs before the C\# compiler, which means the compiler only sees the transformed code and so it points to that in debugging symbols.

Seeing only user code while debugging would be useful, but it is not necessary, so it has not been implemented.

\bigskip

\todo{conclusion}